{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3_TransferLearning_with_Data_Augmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/FormationUnivAngers/blob/main/Jour2/3_TransferLearning_with_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Weeds vs plants Image Classification With Transfer Learning and Image Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "In this tutorial, we will discuss how to classify images into pictures of weeds or pictures of plants. We'll build an image classifier using `tf.keras.Sequential` model and load data using `tf.keras.preprocessing.image.ImageDataGenerator`.\n",
        "\n",
        "## Specific concepts that will be covered:\n",
        "In the process, we will build practical experience and develop intuition around the following concepts\n",
        "\n",
        "* Building _data input pipelines_ using the `tf.keras.preprocessing.image.ImageDataGenerator` class — How can we efficiently work with data on disk to interface with our model?\n",
        "* _Overfitting_ - what is it, how to identify it, and how can we prevent it?\n",
        "* _Data Augmentation_ and _Dropout_ - Key techniques to fight overfitting in computer vision tasks that we will incorporate into our data pipeline and image classifier model.\n",
        "\n",
        "## We will follow the general machine learning workflow:\n",
        "\n",
        "1. Examine and understand data\n",
        "2. Build an input pipeline\n",
        "3. Build our model\n",
        "4. Train our model\n",
        "5. Test our model\n",
        "6. Improve our model/Repeat the process\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Before you begin**\n",
        "\n",
        "Before running the code in this notebook, reset the runtime by going to **Kernel -> Restart & clear output** in the menu above. If you have been working through several notebooks, this will help you avoid reaching memory limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "# Importing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VddxeYBEVrVZ"
      },
      "source": [
        "Let's start by importing required packages:\n",
        "\n",
        "*   os — to read files and directory structure\n",
        "*   numpy — for some matrix math outside of TensorFlow\n",
        "*   matplotlib.pyplot — to plot the graph and display images in our training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPGh2MAVrVa"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in3OdvpUG_9_"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1WtoaOHVrVh"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ede3_kbeHOjR"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z-8RPSC1fIr"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYwWvirp1fIs"
      },
      "source": [
        "To build our image classifier, we begin by loading the dataset. The dataset we are using is consists of weeds and plants.\n",
        "\n",
        "In this Colab, we will make use of the class `tf.keras.preprocessing.image.ImageDataGenerator` which will read data from disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-NN4VfZ1fIx"
      },
      "source": [
        "We'll now assign variables with the proper file path for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r30gx8VAh5Py"
      },
      "source": [
        "# connection between the Colab and the Goggle Drive\n",
        "from google.colab import drive\n",
        "root = '/content/gdrive/'\n",
        "drive.mount( root )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV9t90rph6yB"
      },
      "source": [
        "# create permanent directory in gdrive\n",
        "data_dir_path = r'/My Drive/FormationUA/'\n",
        "os.makedirs(root+data_dir_path, exist_ok=True)\n",
        "os.listdir(root+data_dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRIWOPCFh-fB"
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/FormationUA/data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEpDGnRJiFew"
      },
      "source": [
        "ls data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymZLg7kq1fIy"
      },
      "source": [
        "base_dir = 'data'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'Validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utv3nryxVrV0"
      },
      "source": [
        "train_weeds_dir = os.path.join(train_dir, 'weeds')  # directory with our training w pictures\n",
        "train_plants_dir = os.path.join(train_dir, 'plants')  # directory with our training plant pictures\n",
        "validation_weeds_dir = os.path.join(validation_dir, 'weeds')  # directory with our validation weed pictures\n",
        "validation_plants_dir = os.path.join(validation_dir, 'plants')  # directory with our validation plant pictures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdrHHTy2VrV3"
      },
      "source": [
        "### Understanding our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LblUYjl-VrV3"
      },
      "source": [
        "Let's look at how many ws and plants images we have in our training and validation directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc4u8e9hVrV4"
      },
      "source": [
        "num_weeds_tr = len(os.listdir(train_weeds_dir))\n",
        "num_plants_tr = len(os.listdir(train_plants_dir))\n",
        "\n",
        "num_weeds_val = len(os.listdir(validation_weeds_dir))\n",
        "num_plants_val = len(os.listdir(validation_plants_dir))\n",
        "\n",
        "total_train = num_weeds_tr + num_plants_tr\n",
        "total_val = num_weeds_val + num_plants_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4GGzGt0VrV7"
      },
      "source": [
        "print('total training weeds images:', num_weeds_tr)\n",
        "print('total training plants images:', num_plants_tr)\n",
        "\n",
        "print('total validation weeds images:', num_weeds_val)\n",
        "print('total validation plants images:', num_plants_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdsI_L-NVrV_"
      },
      "source": [
        "# Setting Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-0ejxOtP1"
      },
      "source": [
        "**For** convenience, let us set up variables that will be used later while pre-processing our dataset and training our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NqNselLVrWA"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SHAPE  = 224 # Our training data consists of images with width of 150 pixels and height of 150 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLciCR_FVrWH"
      },
      "source": [
        "After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk and will apply rescaling and will resize them into required dimensions using single line of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOoVpxFwVrWy"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn_QLciWVrWy"
      },
      "source": [
        "Overfitting often occurs when we have a small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number and variety of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples through random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This exposes the model to more aspects of the data, allowing it to generalize better.\n",
        "\n",
        "In **tf.keras** we can implement this using the same **ImageDataGenerator** class we used before. We can simply pass different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process.\n",
        "\n",
        "To start off, let's define a function that can display an image, so we can see the type of augmentation that has been performed. Then, we'll look at specific augmentations that we'll use during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBYLOFgOXPJ9"
      },
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip(images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usS13KCNVrXd"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC8fIsalVrXd"
      },
      "source": [
        "We can apply all these augmentations, and even others, with just one line of code, by passing the augmentations as arguments with proper values.\n",
        "\n",
        "Here, we have applied rescale, rotation of 45 degrees, width shift, height shift, horizontal flip, and zoom augmentation to our training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnr2xujaVrXe"
      },
      "source": [
        "image_gen_train = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                     class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW-pV5awVrXl"
      },
      "source": [
        "Let's visualize how a single image would look like five different times, when we pass these augmentations randomly to our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2m68eMhVrXm"
      },
      "source": [
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8cUd7FXVrXq"
      },
      "source": [
        "### Creating Validation Data generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99fDBt7VrXr"
      },
      "source": [
        "Generally, we only apply data augmentation to our training examples, since the original images should be representative of what our model needs to manage. So, in this case we are only rescaling our validation images and converting them into batches using ImageDataGenerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54x0aNbKVrXr"
      },
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ej-HLGVrWZ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEgW4i18VrWZ"
      },
      "source": [
        "## Transfer learning by starting with existing network\n",
        "\n",
        "Now we can move on to the main strategy for training an image classifier on our small dataset: by starting with a larger and already trained network.\n",
        "\n",
        "To start, we will load the VGG16 from keras, which was trained on ImageNet and the weights saved online. If this is your first time loading VGG16, you'll need to wait a bit for the weights to download from the web. Once the network is loaded, we can again inspect the layers with the `summary()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HThrrNYBZcHG"
      },
      "source": [
        "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPBO6XE9ZcHH"
      },
      "source": [
        "Notice that VGG16 is _much_ bigger than the network we constructed earlier. It contains 13 convolutional layers and two fully connected layers at the end, and has over 138 million parameters, around 100 times as many parameters than the network we made above. Like our first network, the majority of the parameters are stored in the connections leading into the first fully-connected layer.\n",
        "\n",
        "VGG16 was made to solve ImageNet, and achieves a [8.8\\% top-5 error rate](https://github.com/jcjohnson/cnn-benchmarks), which means that 91.2% of test samples were classified correctly within the top 5 predictions for each image. It's top-1 accuracy--equivalent to the accuracy metric we've been using (that the top prediction is correct)--is 73%. This is especially impressive since there are not just 97, but 1000 classes, meaning that random guesses would get us only 0.1% accuracy.\n",
        "\n",
        "In order to use this network for our task, we \"remove\" the final classification layer, the 1000-neuron softmax layer at the end, which corresponds to ImageNet, and instead replace it with a new softmax layer for our dataset, which contains 2 neurons in the case of the weeds and plants dataset. \n",
        "\n",
        "In terms of implementation, it's easier to simply create a copy of VGG from its input layer until the second to last layer, and then work with that, rather than modifying the VGG object directly. So technically we never \"remove\" anything, we just circumvent/ignore it. This can be done in the following way, by using the keras `Model` class to initialize a new model whose input layer is the same as VGG but whose output layer is our new softmax layer, called `new_classification_layer`. Note: although it appears we are duplicating this large network, internally Keras is actually just copying all the layers by reference, and thus we don't need to worry about overloading the memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czIdB-MJZcHH"
      },
      "source": [
        "# make a reference to VGG's input layer\n",
        "inp = vgg.input\n",
        "\n",
        "# make a new softmax layer with num_classes neurons\n",
        "new_classification_layer = tf.keras.layers.Dense(2, activation='softmax')\n",
        "\n",
        "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
        "out = new_classification_layer(vgg.layers[-2].output)\n",
        "\n",
        "# create a new network between inp and out\n",
        "model_new = tf.keras.models.Model(inp, out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA_rV_aaZcHH"
      },
      "source": [
        "We are going to retrain this network, `model_new` on the new dataset and labels. But first, we need to freeze the weights and biases in all the layers in the network, except our new one at the end, with the expectation that the features that were learned in VGG should still be fairly relevant to the new image classification task. Not optimal, but most likely better than what we can train to in our limited dataset. \n",
        "\n",
        "By setting the `trainable` flag in each layer false (except our new classification layer), we ensure all the weights and biases in those layers remain fixed, and we simply train the weights in the one layer at the end. In some cases, it is desirable to *not* freeze all the pre-classification layers. If your dataset has enough samples, and doesn't resemble ImageNet very much, it might be advantageous to fine-tune some of the VGG layers along with the new classifier, or possibly even all of them. To do this, you can change the below code to make more of the layers trainable.\n",
        "\n",
        "In the case of weed and plant database, we will just do feature extraction, fearing that fine-tuning too much with this dataset may overfit. But maybe we are wrong? A good exercise would be to try out both, and compare the results.\n",
        "\n",
        "So we go ahead and freeze the layers, and compile the new model with exactly the same optimizer and loss function as in our first network, for the sake of a fair comparison. We then run `summary` again to look at the network's architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWoZmnUWZcHH"
      },
      "source": [
        "# make all layers untrainable by freezing weights (except for last layer)\n",
        "for l, layer in enumerate(model_new.layers[:-1]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# ensure the last layer is trainable/not frozen\n",
        "for l, layer in enumerate(model_new.layers[-1:]):\n",
        "    layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADWLqMSJcH3"
      },
      "source": [
        "### Compiling the model\n",
        "\n",
        "As usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08rRJ0sn3Tb1"
      },
      "source": [
        "model_new.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uurnCp_H4Hj9"
      },
      "source": [
        "### Model Summary\n",
        "\n",
        "Let's look at all the layers of our network using **summary** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b66qAJF_4Jnw"
      },
      "source": [
        "model_new.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06iqE8VVrWj"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oub9RtoFVrWk"
      },
      "source": [
        "It's time we train our network.\n",
        "\n",
        "Since our batches are coming from a generator (`ImageDataGenerator`), we'll use `fit_generator` instead of `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR31ks0Z1fKH"
      },
      "source": [
        "# saving the log and show it by tensorboard\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "tbcCNN=TensorBoardColab()\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=20, verbose=1, mode='auto') #Stop training when a monitored metric has stopped improving.\n",
        "\n",
        "checkpoint_filepath = 'checkpoint.h5'\n",
        "Model_check = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=False,\n",
        "    save_weights_only=False, mode='auto') #Callback to save the Keras model or model weights at some frequency."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5NT1PW3j_P"
      },
      "source": [
        "epochs=100\n",
        "history = model_new.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))),\n",
        "    callbacks=[TensorBoardColabCallback(tbcCNN),early_stop, Model_check])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-CSbDAoTowv"
      },
      "source": [
        "###Load trained Model###\n",
        "\n",
        "The model can be loaded via the load_model() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyX2Klo0p-I7"
      },
      "source": [
        "# load model\n",
        "model_New = tf.keras.models.load_model('checkpoint.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3eJwpHETwjn"
      },
      "source": [
        "*Prediction* on the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOcOw5BTyiH"
      },
      "source": [
        "# load the image\n",
        "img = cv2.imread('data/Validation/weeds/weed_1.bmp')\n",
        "img = cv2.resize(img, (IMG_SIZE_H, IMG_SIZE_W))\n",
        "img = np.array(img).reshape(-1, IMG_SIZE_H, IMG_SIZE_W, 3)\n",
        "digit = model_New.predict_classes(img)\n",
        "if digit[0] == 0:\n",
        "  print('The predicted class is a Weed')\n",
        "else:\n",
        "  print('The predicted class is a Plant')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}